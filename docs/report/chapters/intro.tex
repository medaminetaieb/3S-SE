\chapter{Introduction}
This chapter is a preamble to the conceptualization of this end-of-studies project. It lays out the context and influences which made it relevant in current settings.\newline
First, it describes the background and sequence of developments that brought much of the employed technologies into existence. Next, it introduces the hosting company and discusses its necessity for a such novel solution. And finally, it walks through the pursued objectives and innovations which address the insufficiencies of existing solutions.
\newpage

\section{Background}
Business Knowledge is both integral and proprietary for any enterprise. The contents of private documents are useful for internal employees who are constantly consuming it to accomplish their workstreams. It becomes evident when considering that modern software development and network infrastructure deployment (among many other domains) are often based on exhaustive documentation and lengthy research papers. Paradoxically, this upfront research and learning introduce a sort of bottleneck requiring much time before a new project gets initiated. In most cases, a further looking up for previously reviewed information is often required. Accelerating delivery, however, remains a key objective for organizations seeking a competitive edge. This together emphasizes the need to consider and propose innovative solutions which would help reduce the preliminary requirements and achieve a faster time-to-market (TTM).\medskip\newline
In line with this discussion, it is worth highlighting the evolving landscape of web search in the contemporary era. We used to input a query into a search engine (e.g. Google) and it would look through its indexed webpages and then return a list of the most relevant pages which we would then read until we have a satisfying answer. This paradigm has veered towards generative AI chatbot solutions, like ChatGPT, Copilot and Gemini, which leverage Machine Learning algorithms to mimic the natural language understanding capability of humans to generate short and accurate answers.\smallskip\newline
Auspiciously, the underlying technology that empower this kind of chatbots is discrete from their corresponding platforms, i.e., it can be integrated in other projects as a library or a software component rather than being exclusive to their native applications. This presents an interesting theme for an internship project which will both provide a much needed solution for an entreprise with many activities and projects to accomplish and for a data science student aspiring to continuously learn the newest trends in AI and Machine Learning.
\newpage

\section{Hosting Company}
The hosting of this project was managed by Standard Sharing Software (3S).
\smallskip\newline
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\linewidth]{./figures/logo_3S.png}
    \caption{Standard Sharing Software logo}
\end{figure}\newline
3S is a Tunisian IT services company specializing in the integration of IT infrastructures. Since its foundation in 1988, it has supported its clients in their digital transformation, management and improvement of their IT infrastructure, from the study to the deployment of the most innovative and high value-added technological solutions.
It is based (headquarters) in Montplaisir, Tunis, Tunisia, and in Charguia 1, Tunis (the office where this internship was conducted), with different teams specializing in Network and Telecommunication infrastructures, Cyber security, and Cloud Computing. This project was organized and executed in the Cloud and Software department of the company, still close collaboration with these different units was required since they constitute the targeted users of this project.
\newpage

\section{Problem Statement}
Traditional enterprise search engines, including the one currently in use by 3S employees, often have many limitations due to the complexity of user queries' context comprehension and human language understanding. This is true because traditional methods of content searching is based on exact text matching without semantic-based retrieval. Even with the emergence of AI tools like ChatGPT, which address Natural Language Processing/Analysis, access to large enterprise data is still a major hurdle for such systems. We have witnessed in recent years the amount of pressure governments have put on AI products in order to evade potential privacy and copyright infringements. This is good overall as such regulations ensure fair competition and publishers' rights. However, this also means that companies need to develop their own solutions to effectively leverage the recent breakthroughs of this field.\medskip\newline
In the table below, an identification and comparison between a few existing solutions has been undertaken to understand their limitations and how to build upon them.
\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \toprule
        \textcolor{darkgray}{\textbf{\makecell{Solution}}}  & \textbf{\makecell{LLM                                                                                                                              \\Chatbots}}                                     & \textbf{\makecell{NotebookLM}}                                       & \textbf{\makecell{Verba}}                                             \\
        \midrule
        \textcolor{darkgray}{\textbf{\makecell{Developer}}} & \makecell{Google, OpenAI                                                                                                                           \\and Microsoft}                                 & Google                                                    & Weaviate                                                   \\
        \midrule
        \textcolor{darkgray}{\textbf{\makecell{RAG-based                                                                                                                                                         \\ (persistent storage)}}} & \textcolor{red}{\ding{56}}        & \textcolor{green}{\ding{52}}                                & \textcolor{green}{\ding{52}}  / \textcolor{red}{\ding{56}} \\
        \midrule
        \textcolor{darkgray}{\textbf{\makecell{Enterprise                                                                                                                                                        \\focused}}} & \textcolor{red}{\ding{56}}                                & \textcolor{red}{\ding{56}}                                & \textcolor{green}{\ding{52}}  / \textcolor{red}{\ding{56}} \\
        \midrule
        \textcolor{darkgray}{\textbf{Extensible}}           & \textcolor{green}{\ding{52}} / \textcolor{red}{\ding{56}} & \textcolor{red}{\ding{56}} & \textcolor{green}{\ding{52}} / \textcolor{red}{\ding{56}} \\
        \midrule
        \textcolor{darkgray}{\textbf{\makecell{Multiple                                                                                                                                                          \\LLMs}}}      & \textcolor{red}{\ding{56}}                                & \textcolor{red}{\ding{56}}                                & \textcolor{green}{\ding{52}}                               \\
        \midrule
        \textcolor{darkgray}{\textbf{\makecell{User-LLM                                                                                                                                                          \\feedback}}}  & \textcolor{green}{\ding{52}} & \textcolor{red}{\ding{56}} / \textcolor{green}{\ding{52}} & \textcolor{red}{\ding{56}}                                 \\
        \midrule
        \textcolor{darkgray}{\textbf{\makecell{Stable                                                                                                                                                            \\ (Not experimental)}}} & \textcolor{green}{\ding{52}} & \textcolor{red}{\ding{56}}                                & \textcolor{red}{\ding{56}}                                 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of existing solutions}
\end{table}\newline
In general, one can utilize any ready-to-use LLM-powered chatbot (e.g. ChatGPT, Microsoft Copilot, Google Gemini, and many others). But these solutions, even though allowing document-answering, don't come with document persistence, which means by leaving the chat, the documents are gone unless re-uploaded.\newline
Google is experimenting with a new solution (NotebookLM) which allows exactly that by connecting to cloud-hosted documents, in addition to methods allowing to upload local files and webpage content from URLs. Yet, this falls short for enterprise use as it is intended for casual use cases (e.g. note taking, document editing suggestions and personal project research) rather than question-answering and does not provide features to share knowledge between different teams. On the other hand, Weaviate, a company behind a vector store implementation, maintains \href{https://github.com/weaviate/Verba}{a repository on GitHub} which addresses the same objectives as this project. Their solution, called Verba, is a well-organized open-source project that can be easily deployed on-premises or in a cloud environment. However, several challenges need to be addressed before a company can adapt it to their specific needs: customization is difficult given that the project uses many dependencies from packages they developed or contributed to (goldenverba, weaviate), requiring many API keys from various providers (vector database, LLMs, cloud providers, etc...), in addition to the inability to create and manage different databases for different teams.\smallskip\newline
This scarcity and inadequacy of available projects raises the need to design and develop a custom solution suitable for the specific needs and requirements set up by individual enterprises (which is articulated in the next section).
\newpage
\section{Objectives}
This project aims to address general-purpose large language models' limitations (data confinement to training phase and private data access) by developing a novel search functionality that leverages retrieval-augmented generation techniques to deliver insightful results for enterprise data. The system will prioritize the following objectives:
\begin{itemize}
    \item Enhance AI-generated responses with External Knowledge: Retrieve relevant passages from a curated enterprise knowledge base each time an LLM is prompted to improve its factual grounding and reduce the risk of hallucinating.
    \item Flexible Data Ingestion and Organization: Implement diverse methods for multi-source, format-agnostic file content and document indexing into manageable and domain-specific vector databases to tailor to enterprise needs and potential evolution.
    \item Diverse Answering Options: Allow the selection from a list of various Large Language Models for answering and just-in-time (of generating responses) data scraping to diversify and improve results each time.
    \item User-Driven Feedback: Integrate a mechanism for providing feedback by users for LLM-generated responses which can be used to rank future results and further train and improve LLMs (when supported).
    \item Leverage RAG-based metrics for evaluation: Employ retrieval-augmented generation evaluation metrics to assess the quality of pipeline stages' results (retrieval quality/accuracy, context-generation consistency, answer relevancy)
\end{itemize}
\newpage
\section{Significance of the study}
The proposed Generative AI solution would help employees find answers from enterprise knowledge without having to browse vast amounts of documents. This is enabled by augmenting LLM knowledge using techniques of retrieval-augmented generation and prompt engineering for context precision. It would also allow them to interfere in the steps of answer-generation by providing ways to customize the external knowledge base and conduct online researching, powered by traditional search engines and generative AI researching tools, through a unified interface.\medskip\newline
From an educational perspective, this project presents a valuable opportunity to gain expertise on some of the most capable ML models and personalize/extend their functioning. Anticipated learning outcomes of accomplishing this project include an enhanced perception of large models' parameters and how they affect performance, picking up prompt engineering techniques, planning out and implementing RAG pipelines that tailor to specific needs, using both local and cloud components, in addition to developing various data scraping methods.