\chapter{Methodology}
This chapter explores the implementation strategies of the system's components and explains the rationale behind the decisions made when different alternatives were available. It then details the project's schedule and anticipated time frames for tasks' completion.
\newpage

\section{Groundwork Analysis}
This section is about researching and exploring the different approaches which can be pursued to implement the various components of the system and choosing the proper way that fits our situation.
\subsection{Cloud providers vs local environment}
The main components of a RAG system (LLMs, a Vector Store, Embedding Model) can be deployed and used both locally or through cloud providers. One should consider the benefits and drawbacks of each method and choose accordingly.
\subsubsection{Vector Store and Embedding Models}
These technologies are coupled together because they complement each other (a vector store depends on an embedding model) and it would be preferable to organize a single environment for both of them. For example, if one or both of these were hosted on the cloud, an internet failure or an access token expiration would result in the overall system outage. It would be wise to avert this downtime by choosing to implement those in a local environment. This would mean that uploading local files will always succeed and network overload or API key expiration risks would be mitigated.\newline
On the other hand, choosing this method would result in an increased hardware utilization (RAM, GPU and Hard Disk) and execution time.\newline
In these circumstances, the advantages of implementing the vector store and embedding model locally outweighs choosing a cloud provider; It minimizes system's downtime, provides more flexibility over choosing the vector store and embedding model (discussed in the next chapter) and minimizes the cost, as embedding and storing large documents is a pricey service.
\subsubsection{Large Language Models}
Contrary to embedding models, LLMs are much more heavier on the hardware and can take long times to generate outputs. This can result in long wait times when an a need for information may be crucial, especially that the system is designed to facilitate access to information. Moreover, combining a question with its retrieved context, in addition to running multiple instances of LLMs in parallel would aggravate these problems.\newline
One instance where a smaller variant of a Large Language Model (Phi-3-mini-4k-instruct) was implemented locally, it was able to generate an answer to a simple question (without being integrated in a RAG pipeline) after a long delay of approximately 15 minutes and failed in a RAG pipeline (took forever until cancelled). In addition to this, another reason behind implementing LLMs through cloud providers was the plethora and competitiveness of these platforms. Many providers (OpenAI and Mistral for example) provide APIs to access their up-to-date models while others provide multiple LLMs through a single API key. These cloud providers also come with free plans, even though with some limits, but that automatically renew each month or so.
\begin{figure}[htbp]
    \begin{subfigure}{.97\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./figures/LLM-Perf-P1.png}
    \end{subfigure}
    \begin{subfigure}{.97\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./figures/LLM-Perf-P2.png}
    \end{subfigure}
    \caption{API Providers and LLM Performance Leaderboard \href{https://huggingface.co/spaces/ArtificialAnalysis/LLM-Performance-Leaderboard}{[ArtificialAnalysis/LLM-Performance-Leaderboard space on Hugging Face]}}.
\end{figure}
\newpage

\subsection{Entreprise Knowledge Base Augmentation}
The main issue with Large Language Models is that they are confined to the data they were trained on. This means they do not have access to information about recent events or private EKB, thus the need to implement methods to allow the continuous upload of documents into the vector store.\newline
In our case, there is no need to collect data as this project does not involve training or fine-tuning models, but rather providing methods that allow scraping and uploading information from various sources.\newline
For this purpose, an anticipatory study of the system's knowledge augmentation methods have resulted in identifying the following requirements :
\begin{itemize}
    \item Offline files: Support for the different offline document formats (text files, pdfs, word documents, powerpoint presentations, e-book format/epub, markdown documentation)
    \item Web Scraping: A single web page from a URL, or recursively scraping its child pages
    \item Search Engine Results: By using some of Search Engine APIs, connect the user's query directly to search results and then scrape information for the returned URLs.
    \item AI-generated researches: LLMs can also automatize the traditional process of searching the web, reading webpages, and artificially generating a report based on those analyzed pages.
\end{itemize}
\newpage

\section{Project Management}
After gaining a better understanding of the elements that would constitute the project it is now time to plan out expected tasks into an organized timetable. This section of the chapter addresses that. It is composed of two parts, one explaining the suitability of the employed methodology throughout the project, and another detailing the tasks to be undertaken.
\subsection{Methodology}
Given the nature of this project, the Scrum methodology was selected for the implementation of the system. This iterative approach allowed for the continuous development and refinement of tasks (Sprints) throughout this internship by allowing users' interference and feedback during the development process. This flexibility was particularly valuable in this end-of-studies project where the final requirements kept evolving.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{./figures/scrum-framework-9.29.23.png}
    \caption{Scrum framework \href{https://www.scrum.org/resources/what-scrum-module}{[What is Scrum (scrum.org)]}}.
\end{figure}\newline
This methodology defines the steps to implement a project as follows :
\begin{enumerate}
    \item Project Planning : Defining the project's overall objectives and values to be gained by users. And on a smaller scale, the specific Sprint objectives.
    \item Product Backlog : Identifying and re-prioritizing the customer's requirements into an organized list of Sprints.
    \item Sprints : Identifying and organizing tasks into a list of Sprints
    \item Scrum Meetings : Daily short meetings to discuss progress and challenges to complete a Sprint's tasks.
    \item Increment development : Iteratively complete Sprint cycles (planning, design, coding, testing and review) to complete increments.
    \item Sprint Review : Review of the completed work with the entreprise supervisor
    \item Sprint Retrospective : Discussing what went good and what went bad during the Sprint to gain better insight on what can be improved in the future.
\end{enumerate}
The following section demonstrates the influence of this agile methodology, by showcasing a planned timetable in compliance with the iterative approach of the Scrum methodology.\newpage
\subsection{Planning}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{./figures/gantt-chart-1.png}
    \caption{The planned Gantt Chart}
\end{figure}
\par This Gantt Chart illustrates the timetable of planned tasks and how they were split over smaller chunks to accomplish them and iterate the development process over different segments. This method allowed to accomplish Increments, which represent tangible steps towards the final project implementation: The first iteration (second week of March) was planned to implement Large Language Models standalone answering functionality (without RAG or a vector store), combining cloud-based LLMs with prompt engineering techniques. The next iteration focused on initiating the RAG pipeline by implementing a vector store index with static content, modifying Prompts to control the LLMs' factual grounding, after which some testing and further development was conducted to tailor to the different LLMs and make the vector store more dynamic by implementing data augmentation methods. After these two iterations which were completed by the start of May, further enhancement was undertaken to customize the indexing methods to tailor to different teams and the retrieval phase to minimize dissimilar results, in addition to supplying more data augmentation knowledge sources through APIs and the exploration of how a ranking algorithm could be implemented. The final iteration focused on integrating all the functionalities into a web interface, implementing a user feedback option and enhance answer ranking and validating the results with the corporate supervisor.

\section{Conclusion}
This chapter, along with the previous one, clarified the project's roadmap and helped breaking down the problem statement into a list of tasks to be implemented. These tasks are now explained in more detail in the following chapter, which discusses the implementation phase of this project.