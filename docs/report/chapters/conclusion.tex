\chapter*{General Conclusion}

This project has resulted in the successful design and implementation of a RAG pipeline. By effectively combining the strengths of LLMs and vector stores, the tool demonstrates the potential to revolutionize how the organization manage and access their information assets.

The RAG pipeline architecture, incorporating data ingestion, preprocessing, embedding generation, vector store indexing, and query processing, has been meticulously designed to ensure optimal performance and accuracy. The employed LLMs, with their advanced language understanding and generation capabilities, have proven to be instrumental in extracting valuable insights from the data. The vector store, efficiently storing and retrieving embeddings, has significantly enhanced the search capabilities of the tool by retrieving relevant contexts.

While the project has achieved its primary objectives, there are opportunities for further exploration and improvement. These include employing local Large Language Models in order to make it possible to fine-tune them based on company data, or to employ Reinforcement Learning from Human Feedback techniques by allowing user interaction with generated responses, or to explore other retrieval techniques, like leveraging some type of LLMs (document-answering task models from Hugging Face Models Hub) which can give more accurate retrieval results. Additionally, a new approach of RAG, called GraphRAG, is currently being developed by Microsoft, which allows to build a graph knowledge base that make the LLMs more capable to extract and understand the relationships between entities from the knowledge base

In conclusion, the implemented RAG-based enterprise knowledge searching tool represents a significant step forward in harnessing the power of AI for information retrieval. It has the potential to streamline workflows, improve decision-making, and unlock new opportunities for the company.